視覚プロンプトと視覚言語モデル（VLMs）を活用し、人間の意図を推定しつつ、人間とロボットの協調作業を通じて共通タスクを達成するための実行可能なロボット行動シーケンスを生成する新たな手法を提案する。
本手法は、一連の注釈（ボックスやラベル）を利用してVLMの環境理解を強化し、現場の環境変化に応じて人間の意図を動的に推測することで、共通目標を達成するための最適なロボット行動シーケンスを生成する。
また、行動の失敗や外部からの干渉が発生した場合には、VLMの分析により、新しいシーケンスを再生成する仕組みを備えている。
